{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning YOLOv2 on LISA Dataset\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "* What is Transfer learning\n",
    "* Relationship between COCO and LISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version) # Check Python Version\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from cfg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LISA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.parse_input import load_data    # Data handler for LISA dataset\n",
    "\n",
    "x_train, y_train = load_data('training.txt')\n",
    "labels           = np.unique(y_train[:,1])\n",
    "num_classes      = len(labels)            # Count number of classes in the dataset\n",
    "\n",
    "print(\"Train: {} samples\\nNumber of classes: {}\".format(len(x_train),num_classes))\n",
    "print(\"\\nLabel Sample: \\n{}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained YOLOv2 on COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.yolov2 import YOLOv2, darknet19\n",
    "import keras.backend as K\n",
    "K.clear_session() # Avoid duplicate model \n",
    "\n",
    "# Original YOLOv2 - Trained on COCO Dataset\n",
    "darknet19 = darknet19(freeze_layers=True)\n",
    "yolov2    = YOLOv2(feature_extractor=darknet19, num_anchors=5, num_classes=80)\n",
    "yolov2.model.load_weights('../yolo-coco.h5')\n",
    "yolov2.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - Cut last layer and create a new Conv2D for Traffic Sign Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from model.net_builder import conv_block\n",
    "# Cut last layer\n",
    "topless_model = yolov2.model.layers[-2].output   \n",
    "\n",
    "# # dd a few new Conv2D layers for Traffic Sign Detection\n",
    "new_conv_block    = conv_block(topless_model, filters=512, kernel_size=(3, 3), padding='same')\n",
    "traffic_sign_detector = Conv2D(filters=(N_ANCHORS* (N_CLASSES + 5)),\n",
    "                               kernel_size=(1, 1), \n",
    "                               kernel_regularizer=l2(5e-4), \n",
    "                               kernel_initializer='he_uniform', activation='linear',\n",
    "                               name='yolov2')(new_conv_block)\n",
    "\n",
    "# Build a new Model for training\n",
    "model = Model(yolov2.model.input, traffic_sign_detector)\n",
    "s\n",
    "#   # # Freeze lower layers\n",
    "for layer in yolov2.model.layers:\n",
    "    layer.trainable = False\n",
    "      \n",
    "model.summary()\n",
    "# for l in model.layers:\n",
    "#     print(l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    pred_shape = K.shape(y_pred)[1:3]\n",
    "    gt_shape   = K.shape(y_true)                 # shape of ground truth value\n",
    "    GRID_H = tf.cast(pred_shape[0], tf.int32)  # shape of output feature map\n",
    "    GRID_W = tf.cast(pred_shape[1], tf.int32)\n",
    "    \n",
    "    output_size = tf.cast(tf.reshape([GRID_W, GRID_H], [1,1,1,1,2]), tf.float32)\n",
    "    y_pred = tf.reshape(y_pred,[-1, pred_shape[0], pred_shape[1], N_ANCHORS, N_CLASSES +5])\n",
    "    y_true = tf.reshape(y_true,[-1, gt_shape[1], gt_shape[2], N_ANCHORS, N_CLASSES + 5])\n",
    "\n",
    "    c_xy = _create_offset_map(K.shape(y_pred))\n",
    "    \n",
    "    pred_box_xy = (tf.sigmoid(y_pred[:,:,:,:,:2]) + c_xy)  / output_size\n",
    "    pred_box_wh = tf.exp(y_pred[:,:,:,:,2:4]) * np.reshape(ANCHORS, [1,1,1,N_ANCHORS,2])\n",
    "    pred_box_wh = tf.sqrt(pred_box_wh / output_size)\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1) \n",
    "    # adjust probability\n",
    "    pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])\n",
    "    \n",
    "    y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "    \n",
    "    ### Adjust ground truth\n",
    "    center_xy = y_true[:,:,:,:,0:2]\n",
    "    true_box_xy = center_xy \n",
    "    true_box_wh = tf.sqrt(y_true[:,:,:,:,2:4])\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_tem_wh = tf.pow(pred_box_wh, 2) *  output_size\n",
    "    pred_box_area = pred_tem_wh[:,:,:,:,0] * pred_tem_wh[:,:,:,:,1]\n",
    "    pred_box_ul = pred_box_xy - 0.5 * pred_tem_wh\n",
    "    pred_box_bd = pred_box_xy + 0.5 * pred_tem_wh\n",
    "    \n",
    "    true_tem_wh = tf.pow(true_box_wh, 2) *  output_size\n",
    "    true_box_area = true_tem_wh[:,:,:,:,0] * true_tem_wh[:,:,:,:,1]\n",
    "    true_box_ul = true_box_xy - 0.5 * true_tem_wh\n",
    "    true_box_bd = true_box_xy + 0.5 * true_tem_wh\n",
    "    \n",
    "    intersect_ul = tf.maximum(pred_box_ul, true_box_ul) \n",
    "    intersect_br = tf.minimum(pred_box_bd, true_box_bd)\n",
    "    intersect_wh = intersect_br - intersect_ul\n",
    "    intersect_wh = tf.maximum(intersect_wh, 0.0)\n",
    "    intersect_area = intersect_wh[:,:,:,:,0] * intersect_wh[:,:,:,:,1]\n",
    "    \n",
    "    iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "    best_box = tf.equal(iou, tf.reduce_max(iou, [3], True)) \n",
    "    best_box = tf.to_float(best_box)\n",
    "    true_box_conf = tf.expand_dims(best_box * y_true[:,:,:,:,4], -1)\n",
    "    \n",
    "    # adjust confidence\n",
    "    true_box_prob = y_true[:,:,:,:,5:]\n",
    "    \n",
    "    y_true = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)\n",
    "    \n",
    "    ### Compute the weights\n",
    "    weight_coor = tf.concat(4 * [true_box_conf], 4)\n",
    "    weight_coor = 5.0 * weight_coor\n",
    "    weight_conf = 0.5 * (1. - true_box_conf) + 5.0 * true_box_conf\n",
    "    weight_prob = tf.concat(N_CLASSES * [true_box_conf], 4) \n",
    "    weight_prob = 1.0 * weight_prob \n",
    "    \n",
    "    weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)\n",
    "    \n",
    "    ### Finalize the loss\n",
    "    loss = tf.pow(y_pred - y_true, 2)\n",
    "    loss = loss * weight                     \n",
    "    loss = tf.reshape(loss, [-1, tf.cast(GRID_W*GRID_H, tf.int32)*N_ANCHORS*(4 + 1 + N_CLASSES)])\n",
    "    loss = tf.reduce_sum(loss, 1)\n",
    "    loss = .5 * tf.reduce_mean(loss)\n",
    "  \n",
    "    return loss\n",
    "\n",
    "def _create_offset_map(output_shape):\n",
    "    \"\"\"\n",
    "    In Yolo9000 paper, grid map\n",
    "    \"\"\"\n",
    "    GRID_H = tf.cast(output_shape[1], tf.int32)  # shape of output feature map\n",
    "    GRID_W = tf.cast(output_shape[2], tf.int32)\n",
    "    \n",
    "    cx = tf.cast((K.arange(0, stop=GRID_W)), dtype=tf.float32)\n",
    "    cx = K.tile(cx, [GRID_H])\n",
    "    cx = K.reshape(cx, [-1, GRID_H, GRID_W, 1])\n",
    "\n",
    "    cy = K.cast((K.arange(0, stop=GRID_H)), dtype=tf.float32)\n",
    "    cy = K.reshape(cy, [-1, 1])\n",
    "    cy = K.tile(cy, [1, GRID_W])  \n",
    "    cy = K.reshape(cy, [-1])    \n",
    "    cy = K.reshape(cy, [-1, GRID_H, GRID_W, 1])\n",
    "\n",
    "    c_xy = tf.stack([cx, cy], -1)\n",
    "    c_xy = K.cast(c_xy, tf.float32)\n",
    "\n",
    "    return c_xy\n",
    "\n",
    "def avg_iou(y_true, y_pred):\n",
    "    pred_shape = K.shape(y_pred)[1:3]\n",
    "    gt_shape   = K.shape(y_true)                 # shape of ground truth value\n",
    "    GRID_H = tf.cast(pred_shape[0], tf.int32)  # shape of output feature map\n",
    "    GRID_W = tf.cast(pred_shape[1], tf.int32)\n",
    "    \n",
    "    output_size = tf.cast(tf.reshape([GRID_W, GRID_H], [1,1,1,1,2]), tf.float32)\n",
    "    y_pred = tf.reshape(y_pred,[-1, pred_shape[0], pred_shape[1], N_ANCHORS, N_CLASSES +5])\n",
    "    y_true = tf.reshape(y_true,[-1, gt_shape[1], gt_shape[2], N_ANCHORS, N_CLASSES + 5])\n",
    "\n",
    "    c_xy = _create_offset_map(K.shape(y_pred))\n",
    "    \n",
    "    pred_box_xy = (tf.sigmoid(y_pred[:,:,:,:,:2]) + c_xy)  / output_size\n",
    "    pred_box_wh = tf.exp(y_pred[:,:,:,:,2:4]) * np.reshape(ANCHORS, [1,1,1,N_ANCHORS,2])\n",
    "    pred_box_wh = tf.sqrt(pred_box_wh / output_size)\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1) \n",
    "    # adjust probability\n",
    "    pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])\n",
    "    \n",
    "    y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "    \n",
    "    ### Adjust ground truth\n",
    "    center_xy = y_true[:,:,:,:,0:2]\n",
    "    true_box_xy = center_xy \n",
    "    true_box_wh = tf.sqrt(y_true[:,:,:,:,2:4])\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_tem_wh = tf.pow(pred_box_wh, 2) *  output_size\n",
    "    pred_box_area = pred_tem_wh[:,:,:,:,0] * pred_tem_wh[:,:,:,:,1]\n",
    "    pred_box_ul = pred_box_xy - 0.5 * pred_tem_wh\n",
    "    pred_box_bd = pred_box_xy + 0.5 * pred_tem_wh\n",
    "    \n",
    "    true_tem_wh = tf.pow(true_box_wh, 2) *  output_size\n",
    "    true_box_area = true_tem_wh[:,:,:,:,0] * true_tem_wh[:,:,:,:,1]\n",
    "    true_box_ul = true_box_xy - 0.5 * true_tem_wh\n",
    "    true_box_bd = true_box_xy + 0.5 * true_tem_wh\n",
    "    \n",
    "    intersect_ul = tf.maximum(pred_box_ul, true_box_ul) \n",
    "    intersect_br = tf.minimum(pred_box_bd, true_box_bd)\n",
    "    intersect_wh = intersect_br - intersect_ul\n",
    "    intersect_wh = tf.maximum(intersect_wh, 0.0)\n",
    "    intersect_area = intersect_wh[:,:,:,:,0] * intersect_wh[:,:,:,:,1]\n",
    "    \n",
    "    iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "    \n",
    "    return tf.reduce_sum(iou) / tf.to_float(gt_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# over fitting on 1 iamge\n",
    "from sklearn.utils import shuffle\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sample = np.tile(x_train[0:4], 2).tolist()\n",
    "one_label  = np.tile(y_train[0:4], [2, 1])\n",
    "print([name.split('/')[-1].split('.')[0] for name in one_sample])\n",
    "print(one_label)\n",
    "print(one_sample[1])\n",
    "# train_data_gen = flow_from_list(one_sample,one_label, batch_size=BATCH_SIZE, augment_data=True)\n",
    "# val_data_gen = flow_from_list(one_sample,one_label, batch_size=BATCH_SIZE, augment_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from utils.multi_gpu import make_parallel, get_gpus\n",
    "from utils.data_generator import flow_from_list\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # HYPER-PARAMETERS\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 1\n",
    "LEARN_RATE = 0.00001 # this model has been pre-trained, LOWER LR is needed\n",
    "# Data Generator\n",
    "train_data_gen = flow_from_list(x_train, y_train, batch_size=BATCH_SIZE, augment_data=True)\n",
    "\n",
    "x_val, y_val = shuffle(x_train, y_train)[0:200]\n",
    "val_data_gen = flow_from_list(x_val, y_val, batch_size=BATCH_SIZE, augment_data=False)\n",
    "\n",
    "train_data_gen = flow_from_list(one_sample,one_label, batch_size=BATCH_SIZE, augment_data=True)\n",
    "val_data_gen = flow_from_list(one_sample,one_label, batch_size=BATCH_SIZE, augment_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "\n",
    "# For Debugging purpose\n",
    "tf_board   = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0000001, patience=3, mode='min', verbose=1)\n",
    "\n",
    "init_loss   = 1000000\n",
    "backup_path = '/home/ubuntu/dataset/backup/'\n",
    "def save_best_model(epoch, logs):\n",
    "    if logs.get('loss') < init_loss:\n",
    "        model.save_weights(backup_path + 'yolov2-epoch%s-loss%s.weights'%(epoch, str(logs.get('loss'))))\n",
    "        \n",
    "save_model = keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: save_best_model(epoch, logs))\n",
    "\n",
    "# Set up Data parallelism\n",
    "n_gpus = get_gpus()\n",
    "if n_gpus > 1:\n",
    "    BATCH_SIZE = n_gpus * BATCH_SIZE\n",
    "    model_par = make_parallel(model, n_gpus)\n",
    "else:\n",
    "    model_par = model\n",
    "\n",
    "# adam = Adam(LEARN_RATE)\n",
    "sgd = SGD(LEARN_RATE, decay=0.0005, momentum=0.9)\n",
    "def schedule(epochs):\n",
    "    if epochs < 90:\n",
    "        return LEARN_RATE\n",
    "    if 90 <= epochs <= 160:\n",
    "        return LEARN_RATE / 10.\n",
    "    if epochs >= 160:\n",
    "        return LEARN_RATE / 100.\n",
    "\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Compiling model\n",
    "model_par.compile(optimizer=sgd,loss=custom_loss, metrics=[avg_iou])\n",
    "\n",
    "hist =  model_par.fit_generator(generator     = train_data_gen, \n",
    "                            steps_per_epoch   = 5, \n",
    "                            validation_data   = val_data_gen,\n",
    "                            validation_steps  = 1,\n",
    "                            epochs            = 300, \n",
    "                            callbacks         = [tf_board, lr_scheduler],\n",
    "                            workers=1, verbose=1)\n",
    "\n",
    "model.save_weights('yolov2.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs Training - Data Parallelism Approach\n",
    "\n",
    "* Each GPU will have a copy of the model\n",
    "* During training time, mean of all gradidents from each GPU will be calculated to update the model\n",
    "<img style=\"width:40%\" src=\"https://www.tensorflow.org/images/Parallelism.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('yolov2.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training process using Tensorboard\n",
    "Open `http://<public-dns>:6006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# samples, labels = train_data_gen.next()\n",
    "# i = 2\n",
    "# img    = samples[i]\n",
    "# labels = labels.reshape([-1, 30, 40, 5, 36])\n",
    "# box    = labels[i][8][30][0][:4]*np.array([1280, 960, 1280, 960])\n",
    "# print(box)\n",
    "\n",
    "# dot    = cv2.circle(img, (int(box[0]), int(box[1])), radius=5, color=[0, 1, 0], thickness=5)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def extract_sign(img, bbox, output_size=(32, 32)):\n",
    "#     xc, yc, w, h = bbox.x, bbox.y, bbox.w, bbox.h\n",
    "#     x1   = int(xc - w/2)\n",
    "#     y1   = int(yc - h/2)\n",
    "#     x2   = int(xc + w/2)\n",
    "#     y2   = int(yc + h/2)\n",
    "#     roi = cv2.resize(img[y1:y2, x1:x2], output_size)\n",
    "#     return roi\n",
    "\n",
    "# x_train, y_train = shuffle(x_train, y_train)\n",
    "# fig = plt.figure(figsize=(17, 8))\n",
    "# for i, label in enumerate(labels):\n",
    "#     ax           = fig.add_subplot(4, 8, 1 + i, xticks=[], yticks=[])\n",
    "#     idx          = np.where(y_train[:, 1] == label)[0][0]\n",
    "#     img          = cv2.cvtColor(cv2.imread(x_train[idx]), cv2.COLOR_BGR2RGB)\n",
    "#     box          = y_train[idx][0]\n",
    "#     sign_only    = extract_sign(img, box, (32, 32)) #  just extract the sign\n",
    "#     ax.set_title(label)\n",
    "#     plt.imshow(sign_only)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
