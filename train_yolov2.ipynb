{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLOv2 on LISA Dataset\n",
    "-----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # Check Python Version\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from utils.parse_input import load_data    # Data handler for LISA dataset\n",
    "from cfg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LISA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth boxes: 3672 boxes\n",
      "Train: 3672 samples\n",
      "Number of classes: 31\n",
      "\n",
      "\n",
      "Anchors using K-mean clustering [K=5]\n",
      " [[ 2.34451923  2.58625   ]\n",
      " [ 0.94216296  1.06618128]\n",
      " [ 3.51704545  3.86079545]\n",
      " [ 1.28419918  1.41645871]\n",
      " [ 1.72788361  1.87969729]]\n"
     ]
    }
   ],
   "source": [
    "lisa_path = \"/home/ubuntu/dataset/training/\" # Remember the `/` at the end\n",
    "pretrained_path = \"/home/ubuntu/dataset/darknet19_544.weights\"\n",
    "x_train, y_train = load_data('training.txt')\n",
    "labels           = np.unique(y_train[:,1])\n",
    "num_classes      = len(labels)            # Count number of classes in the dataset\n",
    "print(\"Train: {} samples\\nNumber of classes: {}\".format(len(x_train),num_classes))\n",
    "\n",
    "print(\"\\n\\nAnchors using K-mean clustering [K=5]\\n {}\".format(ANCHORS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct YOLOv2 On Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights have been loaded into model\n"
     ]
    }
   ],
   "source": [
    "from yolov2.model import YOLOv2, darknet19\n",
    "darknet19 = darknet19(pretrained_path, freeze_layers=True)\n",
    "yolov2    = YOLOv2(feature_extractor=darknet19, num_anchors=len(ANCHORS), num_classes=N_CLASSES)\n",
    "model     = yolov2.model\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    NORM_W = 960\n",
    "    NORM_H = 1280\n",
    "    GRID_W = 30\n",
    "    GRID_H = 40\n",
    "    \n",
    "    no_object_scale = 0.5\n",
    "    object_scale     = 5.0\n",
    "    coordinate_scale = 5.0\n",
    "    class_scale      = 1.0\n",
    "    N_ANCHORS        = len(ANCHORS)\n",
    "\n",
    "    pred_shape = K.shape(y_pred)[1:3]\n",
    "    y_pred = K.reshape(y_pred, [-1, pred_shape[0], pred_shape[1], N_ANCHORS, N_CLASSES + 5])\n",
    "    y_true = K.reshape(y_true, [-1, pred_shape[0], pred_shape[1], N_ANCHORS, N_CLASSES + 5])\n",
    "\n",
    "    anchor_tensor = np.reshape(ANCHORS, [1, 1, 1, N_ANCHORS, 2])\n",
    "\n",
    "    #  Adjust Prediction\n",
    "    pred_box_xy   = tf.sigmoid(y_pred[:, :, :, :, :2])\n",
    "    pred_box_wh   = tf.exp(y_pred[:, :, :, :, 2:4]) * anchor_tensor\n",
    "    pred_box_wh   = tf.sqrt(pred_box_wh / np.reshape([float(GRID_W), float(GRID_H)], [1, 1, 1, 1, 2]))\n",
    "    pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1)  # adjust confidence\n",
    "    pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])  # adjust probability\n",
    "\n",
    "    #  Adjust ground truth\n",
    "    center_xy = y_true[:, :, :, :, 0:2]\n",
    "    center_xy = center_xy / np.reshape([(float(NORM_W) / GRID_W), (float(NORM_H) / GRID_H)], [1, 1, 1, 1, 2])\n",
    "\n",
    "    true_box_xy = center_xy - tf.floor(center_xy)\n",
    "    true_box_wh = y_true[:, :, :, :, 2:4]\n",
    "    true_box_wh = tf.sqrt(true_box_wh / np.reshape([float(NORM_W), float(NORM_H)], [1, 1, 1, 1, 2]))\n",
    "\n",
    "    # adjust confidence\n",
    "    pred_tem_wh = tf.pow(pred_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1, 1, 1, 1, 2])\n",
    "    pred_box_area = pred_tem_wh[:, :, :, :, 0] * pred_tem_wh[:, :, :, :, 1]\n",
    "    pred_box_ul = pred_box_xy - 0.5 * pred_tem_wh\n",
    "    pred_box_bd = pred_box_xy + 0.5 * pred_tem_wh\n",
    "\n",
    "    true_tem_wh = tf.pow(true_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1, 1, 1, 1, 2])\n",
    "    true_box_area = true_tem_wh[:, :, :, :, 0] * true_tem_wh[:, :, :, :, 1]\n",
    "    true_box_ul = true_box_xy - 0.5 * true_tem_wh\n",
    "    true_box_bd = true_box_xy + 0.5 * true_tem_wh\n",
    "\n",
    "    # Calculate IoU between ground truth and prediction\n",
    "    intersect_ul = tf.maximum(pred_box_ul, true_box_ul)\n",
    "    intersect_br = tf.minimum(pred_box_bd, true_box_bd)\n",
    "    intersect_wh = intersect_br - intersect_ul\n",
    "    intersect_wh = tf.maximum(intersect_wh, 0.0)\n",
    "    intersect_area = intersect_wh[:, :, :, :, 0] * intersect_wh[:, :, :, :, 1]\n",
    "\n",
    "    iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "    best_box = tf.equal(iou, tf.reduce_max(iou, [3], True))\n",
    "    best_box = tf.to_float(best_box)\n",
    "    # calculate avg_iou here\n",
    "\n",
    "    true_box_conf = tf.expand_dims(best_box * y_true[:, :, :, :, 4], -1)\n",
    "    true_box_prob = y_true[:, :, :, :, 5:]  # adjust confidence\n",
    "\n",
    "    y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "    y_true = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)\n",
    "\n",
    "    # Compute the weights\n",
    "\n",
    "    # Object Confidence Loss\n",
    "    weight_conf = no_object_scale * (1. - true_box_conf) + object_scale * true_box_conf\n",
    "\n",
    "    # Object Localization Loss\n",
    "    weight_coor = tf.concat(4 * [true_box_conf], 4)\n",
    "    weight_coor = coordinate_scale * weight_coor\n",
    "\n",
    "    # Object Classification Loss\n",
    "    weight_prob = tf.concat(N_CLASSES * [true_box_conf], 4)\n",
    "    weight_prob = class_scale * weight_prob\n",
    "\n",
    "    # Total Loss\n",
    "    weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)\n",
    "\n",
    "    # ## Finalize the loss\n",
    "    loss = tf.pow(y_pred - y_true, 2)\n",
    "    loss = loss * weight\n",
    "    loss = tf.reshape(loss, [-1, GRID_W * GRID_H * N_ANCHORS * (4 + 1 + N_CLASSES)])\n",
    "    loss = tf.reduce_sum(loss, 1)\n",
    "    loss = .5 * tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  2/229 [..............................] - ETA: 2429s - loss: 424.1454"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras\n",
    "from utils.data_generator import flow_from_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# HYPER-PARAMETERS\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 5\n",
    "LEARN_RATE = 1e-5\n",
    "\n",
    "\n",
    "# Data Generator\n",
    "train_data_gen = flow_from_list(x_train, y_train, ANCHORS, batch_size=BATCH_SIZE, augment_data=False)\n",
    "\n",
    "# For Debugging purpose\n",
    "tf_board = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "check_pt = keras.callbacks.ModelCheckpoint('models/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                                           verbose=0, save_best_only=False, \n",
    "                                           save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "model.compile(optimizer=Adam(LEARN_RATE),loss=custom_loss)\n",
    "hist =  model.fit_generator(generator       = train_data_gen, \n",
    "                            steps_per_epoch = len(x_train) / BATCH_SIZE, \n",
    "                            epochs          = EPOCHS, \n",
    "                            callbacks       = [tf_board, check_pt],\n",
    "                            workers=1, verbose=1)\n",
    "\n",
    "model.save_weights('darknet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs Training - Data Parallelism Approach\n",
    "\n",
    "* Each GPU will have a copy of the model\n",
    "* During training time, mean of all gradidents from each GPU will be calculated to update the model\n",
    "<img style=\"width:40%\" src=\"https://www.tensorflow.org/images/Parallelism.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training process using Tensorboard\n",
    "Open `http://<public-dns>:6006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def extract_sign(img, bbox, output_size=(32, 32)):\n",
    "#     xc, yc, w, h = bbox.x, bbox.y, bbox.w, bbox.h\n",
    "#     x1   = int(xc - w/2)\n",
    "#     y1   = int(yc - h/2)\n",
    "#     x2   = int(xc + w/2)\n",
    "#     y2   = int(yc + h/2)\n",
    "#     roi = img[y1:y2, x1:x2]\n",
    "#     roi = cv2.resize(roi, output_size)\n",
    "#     return roi\n",
    "# x_train, y_train = shuffle(x_train, y_train)\n",
    "# fig = plt.figure(figsize=(17, 8))\n",
    "# for i, label in enumerate(labels):\n",
    "#     ax           = fig.add_subplot(4, 8, 1 + i, xticks=[], yticks=[])\n",
    "#     idx          = np.where(y_train[:, 1] == label)[0][0]\n",
    "#     img          = cv2.cvtColor(cv2.imread(x_train[idx]), cv2.COLOR_BGR2RGB)\n",
    "#     box          = y_train[idx][0]\n",
    "#     sign_only    = extract_sign(img, box, (32, 32)) #  just extract the sign\n",
    "#     ax.set_title(label)\n",
    "#     plt.imshow(sign_only)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Count frequencies of each traffic sign\n",
    "# labels, frequencies = np.unique(y_train[:,1 ], return_counts=True)\n",
    "\n",
    "# plt.figure(figsize=(20, 5))\n",
    "# x = np.arange(len(labels))\n",
    "# plt.xticks(x, labels, rotation=70)\n",
    "# plt.bar(x, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "    \n",
    "#     N_ANCHORS = len(ANCHORS)\n",
    "    \n",
    "#     NORM_W, NORM_H = GRID_W * 32, GRID_H * 32      # Scale back to get image input size\n",
    "#     SCALE_NOOB = 0.5\n",
    "#     SCALE_CONF = 5.0\n",
    "#     SCALE_COOR = 5.0\n",
    "#     SCALE_PROB = 1.0\n",
    "\n",
    "#     output_shape  = K.shape(y_pred)[0:3] # [batch, width, height]\n",
    "#     grid_map      = K.cast(K.reshape([float(GRID_W), float(GRID_H)], [1, 1, 1, 1, 2]), K.dtype(y_pred))\n",
    "#     anchor_tensor = K.reshape(K.variable(ANCHORS), [1, 1, 1, N_ANCHORS, 2])\n",
    "#     y_true        = K.tile(y_true, [1, output_shape[0], output_shape[1], N_ANCHORS])\n",
    "\n",
    "#     y_pred        = K.reshape(y_pred, [-1, output_shape[0], output_shape[1], N_ANCHORS, N_CLASSES + 5])\n",
    "#     y_true        = K.reshape(y_true, [-1, output_shape[0], output_shape[1], N_ANCHORS, N_CLASSES + 5])\n",
    "\n",
    "\n",
    "#     # adjust x and y      \n",
    "#     pred_box_xy   = tf.sigmoid(y_pred[...,:2])\n",
    "#     pred_box_wh   = tf.sqrt((tf.exp(y_pred[...,2:4]) * anchor_tensor)/  grid_map)\n",
    "#     pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[..., 4]), -1)\n",
    "#     pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])\n",
    "    \n",
    "#     ### Adjust ground truth\n",
    "#     # adjust x and y\n",
    "#     center_xy   = y_true[...,:2]\n",
    "#     true_box_xy = center_xy - tf.floor(center_xy)\n",
    "#     true_box_wh = y_true[...,2:4]\n",
    "    \n",
    "#     # adjust confidence\n",
    "#     pred_tem_wh   = tf.pow(pred_box_wh, 2) *  grid_map\n",
    "#     pred_box_area = pred_tem_wh[..., 0] * pred_tem_wh[..., 1]\n",
    "#     pred_box_ul   = pred_box_xy - 0.5   * pred_tem_wh\n",
    "#     pred_box_bd   = pred_box_xy + 0.5   * pred_tem_wh\n",
    "    \n",
    "#     true_tem_wh   = tf.pow(true_box_wh, 2) *  grid_map\n",
    "#     true_box_area = true_tem_wh[...,0] * true_tem_wh[...,1]\n",
    "#     true_box_ul   = true_box_xy - 0.5  * true_tem_wh\n",
    "#     true_box_bd   = true_box_xy + 0.5  * true_tem_wh\n",
    "    \n",
    "#     intersect_ul   = tf.maximum(pred_box_ul, true_box_ul) \n",
    "#     intersect_br   = tf.minimum(pred_box_bd, true_box_bd)\n",
    "#     intersect_wh   = intersect_br - intersect_ul\n",
    "#     intersect_wh   = tf.maximum(intersect_wh, 0.0)\n",
    "#     intersect_area = intersect_wh[...,0] * intersect_wh[...,1]\n",
    "    \n",
    "#     iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "    \n",
    "#     best_box      = tf.equal(iou, tf.reduce_max(iou, [3], True)) \n",
    "#     best_box      = tf.to_float(best_box)\n",
    "#     true_box_conf = tf.expand_dims(best_box * y_true[...,4], -1)  # Prob(obj)*IOU(obj) = sigmoid\n",
    "#     true_box_prob = y_true[...,5:]    # adjust confidence\n",
    "    \n",
    "#     y_true = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)\n",
    "#     y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "\n",
    "#     ### Compute the weights\n",
    "#     weight_coor = tf.concat(4 * [true_box_conf], 4)\n",
    "#     weight_coor = SCALE_COOR * weight_coor\n",
    "#     weight_conf = SCALE_NOOB * (1. - true_box_conf) + SCALE_CONF * true_box_conf\n",
    "#     weight_prob = tf.concat(N_CLASSES * [true_box_conf], 4) \n",
    "#     weight_prob = SCALE_PROB * weight_prob \n",
    "    \n",
    "#     weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)\n",
    "#     ### Finalize the loss\n",
    "#     loss = tf.pow(y_pred - y_true, 2)\n",
    "\n",
    "#     loss = loss * weight\n",
    "\n",
    "#     loss = weight\n",
    "\n",
    "#     loss = tf.reshape(loss, [-1, GRID_W*GRID_H*N_ANCHORS*(4 + 1 + N_CLASSES)])\n",
    "\n",
    "#     loss = tf.reduce_sum(loss, 1)\n",
    "\n",
    "#     loss = .5 * tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
