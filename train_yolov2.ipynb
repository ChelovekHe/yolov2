{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLOv2 on LISA Dataset\n",
    "-----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # Check Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from utils.parse_input import load_data    # Data handler for LISA dataset\n",
    "from yolov2.model import YOLOv2, darknet19\n",
    "from utils.data_generator import flow_from_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LISA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lisa_path = \"/home/dat/Downloads/training/\" # Remember the `/` at the end\n",
    "# pretrained_path = \"/home/ubuntu/dataset/darknet19_544.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth boxes: 3672 boxes\n",
      "Train: 3672 samples\n",
      "Number of classes: 31\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_data('training.txt')\n",
    "labels           = np.unique(y_train[:,1])\n",
    "num_classes      = len(labels)            # Count number of classes in the dataset\n",
    "\n",
    "print(\"Train: {} samples\\nNumber of classes: {}\".format(len(x_train),num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct YOLOv2 On Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "darknet19 = darknet19(freeze_layers=True)\n",
    "yolov2    = YOLOv2(feature_extractor=darknet19, num_anchors=5, num_classes=31)\n",
    "model     = yolov2.model\n",
    "\n",
    "# print(model.summary())\n",
    "# HYPER-PARAMETERS\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 5\n",
    "LEARN_RATE = 0.01\n",
    "anchors    = np.array(((0.57273, 0.677385),\n",
    "                     (1.87446, 2.06253),\n",
    "                     (3.33843, 5.47434),\n",
    "                     (7.88282, 3.52778),\n",
    "                     (9.77052, 9.16828)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def yolo_loss(anchors, n_classes):\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "\n",
    "#         CONV_SHAPE = tf.cast(K.shape(y_pred)[1:3], dtype=tf.int32)\n",
    "\n",
    "        GRID_W, GRID_H     = 40, 30\n",
    "#         GRID_W, GRID_H = CONV_SHAPE[0], CONV_SHAPE[1]  # a tiny hack to get width, height of output layer\n",
    "        NORM_W, NORM_H = GRID_W * 32, GRID_H * 32      # Scale back to get image input size\n",
    "\n",
    "        SCALE_NOOB, SCALE_CONF, SCALE_COOR, SCALE_PROB = 0.5, 5.0, 5.0, 1.0\n",
    "    \n",
    "        y_pred = K.reshape(y_pred, [-1, GRID_W, GRID_H, len(anchors), n_classes + 5])\n",
    "        # Adjust prediction  : bx = sigmoid(tx) + cx\n",
    "        pred_box_xy = tf.sigmoid(y_pred[:, :, :, :, :2])\n",
    "\n",
    "        # adjust w and h : bw = pw*exp(tw)\n",
    "        pred_box_wh   = tf.exp(y_pred[:, :, :, :, 2:4]) * np.reshape(anchors, [1, 1, 1, len(anchors), 2])\n",
    "        \n",
    "        # scaling wh relative to top-left corner of feature map\n",
    "        pred_box_wh   = tf.sqrt(pred_box_wh / np.reshape([float(GRID_W), float(GRID_H)], [1, 1, 1, 1, 2]))\n",
    "        pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1)         # adjust confidence\n",
    "        pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])          # adjust probability\n",
    "\n",
    "        y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "\n",
    "        # Adjust ground truth\n",
    "        # adjust x and y\n",
    "        gt_dim    = K.shape(y_true)\n",
    "        y_true    = K.reshape(y_true, [gt_dim[0], GRID_W, GRID_H, gt_dim[1], gt_dim[2]])\n",
    "        center_xy = .5 * (y_true[:, :, :, :, 0:2] + y_true[:, :, :, :, 2:4])\n",
    "        center_xy = center_xy / np.reshape([(float(NORM_W) / GRID_W), (float(NORM_H) / GRID_H)], [1, 1, 1, 1, 2])\n",
    "        true_box_xy = center_xy - tf.floor(center_xy)\n",
    "\n",
    "        # adjust w and h\n",
    "        true_box_wh = (y_true[:, :, :, :, 2:4] - y_true[:, :, :, :, 0:2])\n",
    "        true_box_wh = tf.sqrt(true_box_wh / np.reshape([float(NORM_W), float(NORM_H)], [1, 1, 1, 1, 2]))\n",
    "\n",
    "        # adjust confidence\n",
    "        pred_tem_wh   = tf.pow(pred_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1, 1, 1, 1, 2])\n",
    "        pred_box_area = pred_tem_wh[:, :, :, :, 0] * pred_tem_wh[:, :, :, :, 1]\n",
    "        pred_box_ul   = pred_box_xy - 0.5 * pred_tem_wh\n",
    "        pred_box_bd   = pred_box_xy + 0.5 * pred_tem_wh\n",
    "\n",
    "        true_tem_wh   = tf.pow(true_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1, 1, 1, 1, 2])\n",
    "        true_box_area = true_tem_wh[:, :, :, :, 0] * true_tem_wh[:, :, :, :, 1]\n",
    "        true_box_ul   = true_box_xy - 0.5 * true_tem_wh  # upper left\n",
    "        true_box_br   = true_box_xy + 0.5 * true_tem_wh  # lower right\n",
    "\n",
    "        # Find the IOU between GT and prediction\n",
    "        intersect_ul   = tf.maximum(pred_box_ul, true_box_ul)\n",
    "        intersect_br   = tf.minimum(pred_box_bd, true_box_br)\n",
    "        intersect_wh   = intersect_br - intersect_ul\n",
    "        intersect_wh   = tf.maximum(intersect_wh, 0.0)\n",
    "        intersect_area = intersect_wh[:, :, :, :, 0] * intersect_wh[:, :, :, :, 1]\n",
    "\n",
    "        iou           = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "        best_box      = tf.equal(iou, tf.reduce_max(iou, [3], True))\n",
    "        best_box      = tf.to_float(best_box)\n",
    "        true_box_conf = tf.expand_dims(best_box * y_true[:, :, :, :, 4], -1)\n",
    "\n",
    "        # adjust confidence\n",
    "        true_box_prob = y_true[:, :, :, :, 5:]\n",
    "        y_true        = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)\n",
    "        # y_true = tf.Print(y_true, [true_box_wh], message='DEBUG', summarize=30000)\n",
    "\n",
    "        # ## Compute the weights\n",
    "        weight_coor = tf.concat(4 * [true_box_conf], 4)\n",
    "        weight_coor = SCALE_COOR * weight_coor\n",
    "        weight_conf = SCALE_NOOB * (1. - true_box_conf) + SCALE_CONF * true_box_conf\n",
    "        weight_prob = tf.concat(n_classes* [true_box_conf], 4)\n",
    "        weight_prob = SCALE_PROB * weight_prob\n",
    "        weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)\n",
    "\n",
    "        # ## Finalize the loss\n",
    "        loss = tf.pow(y_pred - y_true, 2)\n",
    "        loss = loss * weight\n",
    "        loss = tf.reshape(loss, [-1, GRID_W * GRID_H * len(anchors) * (4 + 1 + n_classes)])\n",
    "        loss = tf.reduce_sum(loss, 1)\n",
    "        loss = .5 * tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "# from yolov2.loss import yolo_loss\n",
    "\n",
    "train_data_gen = flow_from_list(x_train, y_train, anchors, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer= Adam(LEARN_RATE),\n",
    "              loss     = yolo_loss(anchors, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_board' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-38df1db99f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                             \u001b[0minitial_epoch\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                             callbacks=[tf_board, check_pt])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'darknet_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_board' is not defined"
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(generator       = train_data_gen, \n",
    "                            steps_per_epoch = len(x_train) / BATCH_SIZE, \n",
    "                            epochs          = 10, \n",
    "                            workers=1, \n",
    "                            pickle_safe=False, \n",
    "                            verbose=1, \n",
    "                            initial_epoch   = 0,\n",
    "                            callbacks=[tf_board, check_pt])\n",
    "model.save_weights('darknet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs Training - Data Parallelism Approach\n",
    "\n",
    "* Each GPU will have a copy of the model\n",
    "* During training time, mean of all gradidents from each GPU will be calculated to update the model\n",
    "<img style=\"width:40%\" src=\"https://www.tensorflow.org/images/Parallelism.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training process using Tensorboard\n",
    "Open `http://<public-dns>:6006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def extract_sign(img, bbox, output_size=(32, 32)):\n",
    "    xc, yc, w, h = bbox.x, bbox.y, bbox.w, bbox.h\n",
    "    x1   = int(xc - w/2)\n",
    "    y1   = int(yc - h/2)\n",
    "    x2   = int(xc + w/2)\n",
    "    y2   = int(yc + h/2)\n",
    "    roi = img[y1:y2, x1:x2]\n",
    "    roi = cv2.resize(roi, output_size)\n",
    "    return roi\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "fig = plt.figure(figsize=(17, 8))\n",
    "for i, label in enumerate(labels):\n",
    "    ax           = fig.add_subplot(4, 8, 1 + i, xticks=[], yticks=[])\n",
    "    idx          = np.where(y_train[:, 1] == label)[0][0]\n",
    "    img          = cv2.cvtColor(cv2.imread(x_train[idx]), cv2.COLOR_BGR2RGB)\n",
    "    box          = y_train[idx][0]\n",
    "    sign_only    = extract_sign(img, box, (32, 32)) #  just extract the sign\n",
    "    ax.set_title(label)\n",
    "    plt.imshow(sign_only)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Count frequencies of each traffic sign\n",
    "labels, frequencies = np.unique(y_train[:,1 ], return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "x = np.arange(len(labels))\n",
    "plt.xticks(x, labels, rotation=70)\n",
    "plt.bar(x, frequencies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
